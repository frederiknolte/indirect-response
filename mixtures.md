# Circa Mixtures

This document describes the different mixture tasks we use to finetune T5 to produce accurate classification of indirect answers as well as plausible and faithful rationales. Concretely, we finetune on the train portions of the Cos-E, e-SNLI, and Circa dataset. Finally, we test the knowledge transferability of our mixture on the held out Circa test set.

## NLI Setting

Like Louis et al. 2020, we explore the treatment of indirect answer classification as a Natural Language Inference (NLI) problem.

Mixtures are named by random seed, setting (e.g., `relaxed` or `strict` and `matched` or `unmatched`). The prefix `esnli_and_cos_e_to_circa_nli` implies that we are learning from the e-SNLI and cos-E datasets and transferring that knowledge to the Circa dataset.  **Currently, we only have mixtures for the matched setting**

All available Circa NLI Mixtures:

```code
esnli_and_cos_e_to_circa_nli_relaxed_matched13
esnli_and_cos_e_to_circa_nli_relaxed_matched948
esnli_and_cos_e_to_circa_nli_relaxed_matched2756
esnli_and_cos_e_to_circa_nli_strict_matched13
esnli_and_cos_e_to_circa_nli_strict_matched948
esnli_and_cos_e_to_circa_nli_strict_matched2756

```

### Tasks

1. Predicting e-SNLI instances

    In this task, T5 is charged with predicted the correct label to an NLI premise/hypothesis pair. This improves predictive power.

    Sample input and target:

    ```code
    input: nli premise: A man in an orange vest leans over a pickup truck. hypothesis: A man is touching a truck.
    target: entailment
    ```

    Metrics:

     - Accuracy

2. Predicting and Explaining e-SNLI instances

    In this task, the model is tasked with predicted the label **and** providing an explanation. Since the e-SNLI dataset has reference explanations, we can also improve the quality of the explanations generated by T5.

    Sample input and target:

    ```code
    input: nli premise: A man in an orange vest leans over a pickup truck. hypothesis: A man is touching a truck.
    target: entailment explanation: Man leans over a pickup truck implies that he is touching it.
    ```

    Metrics

    - Accuracy
    - BLEU Score

3. Explaining Cos-E instances

    Per Narang et al. 2020, we modify the standard T5 Cos-E input to match the NLI keywords. It isn't a true NLI task, but this way we can still leverage the wealth of explanations in Cos-E to improve our model.

    Sample input and target:

    ```code
      inputs: explain nli premise: Where would I not want a fox?
                                choice: hen house
                                choice: england
                                choice: mountains
                                choice: english hunt
                                choice: california

      target: hen house explanation: Foxes kill hens.
    ```

    Metrics

    - BLEU Score

4. Predicting Circa instances

    Like Louis et al. 2020, we map the circa labels to the standard NLI ones:

    - no → contradiction
    - yes → entailment
    - in the middle → neutral

    We then use the declarative form of the question as the premise and the candidate answer as the hypothesis.

    The model can't achieve 100% accuracy in this manner, but it can score as high as 88.% in the relaxed setting.

    Sample input and target:

    ```code
    input: nli premise: I have a house: hypothesis: We are in a 9th floor apartment.
    target: contradiction
    ```

    Metrics

    - Accuracy

## QA Setting

We also explore the treatment of indirect answer classification as a Question Answering (QA) problem.

Mixtures are named by seed, setting (e.g., `relaxed` or `strict` and `matched` or `unmatched`). The prefix `esnli_and_cos_e_to_circa_nli` implies that we are learning from the e-SNLI and cos-E datasets and transferring that knowledge to the Circa dataset. **Currently, we only have mixtures for the matched setting**

All available Circa NLI Mixtures:

```code
esnli_and_cos_e_to_circa_qa_relaxed_matched13
esnli_and_cos_e_to_circa_qa_relaxed_matched948
esnli_and_cos_e_to_circa_qa_relaxed_matched2756
esnli_and_cos_e_to_circa_qa_strict_matched13
esnli_and_cos_e_to_circa_qa_strict_matched948
esnli_and_cos_e_to_circa_qa_strict_matched2756
```

1. Predicting e-SNLI instances

    In this task, T5 is charged with predicted the correct answer to an e-SNLI instance when given the NLI labels as choices. This improves predictive power.

    Sample input and target:

    ```code
    input: nli premise: A man in an orange vest leans over a pickup truck. hypothesis: A man is touching a truck.
                choice: entailment
                choice: contradiction
                choice: neutral
    target: entailment
    ```

    Metrics:

    - Accuracy

2. Predicting and Explaining e-SNLI instances

    Same as above, but we ask T5 for an explanation as well.

    Sample input and target:

    ```code
    input: nli premise: A man in an orange vest leans over a pickup truck. hypothesis: A man is touching a truck.
                choice: entailment
                choice: contradiction
                choice: neutral
    target: entailment explanation: Man leans over a pickup truck implies that he is touching it.
    ```

    Metrics

    - Accuracy
    - BLEU Score

3. Predicting Cos-E instances

    Same as above, but we ask T5 for an explanation as well.

    Sample input and target:

    ```code
      inputs: explain cos_e question: Where would I not want a fox?
                    choice: hen house
                    choice: england
                    choice: mountains
                    choice: english hunt
                    choice: california

      target: hen house explanation: Foxes kill hens.
    ```

    Metrics

    - Accuracy
    - BLEU Score

4. Predicting and Explaining Cos-E instances

    T5 is tasked with predicting the correct answer to a Cos-E instance. We follow Narang et al. 2020, by including a special keyword for each QA dataset.

    Sample input and target:

    ```code
      inputs: explain cos_e question: Where would I not want a fox?
                            choice: hen house
                            choice: england
                            choice: mountains
                            choice: english hunt
                            choice: california

      target: hen house
    ```

    Metrics

    - Accuracy
    - Weighted F1

5. Predicting Circa instances

    We treat each possible label in the Circa dataset as a choice. We add the candidate answer with a special keyword. The choices depend on the setting (`relaxed` or `strict1`).

    Sample input and target in the relaxed setting:

    ```code
        inputs: explain circa question: Do you see yourself raising a family in New York? answer: I will never have a family.
                                choice: Yes
                                choice: Yes, subject to some conditions
                                choice: No
                                choice: In the middle, neither yes nor no
                                choice: NA
                                choice: Other
        targets: No
    ```

    Metrics

    - Accuracy
    - Weighted F1

## Final Evaluation

We check how well each mixture prepares the T5 model to explain the reasoning behind each indirect answer classification.

Sample input and target in the NLI setting:

```code
input: nli premise: I have a house: hypothesis: We are in a 9th floor apartment.
target: contradiction explanation: Something here...
```

Sample input and target in the QA `relaxed` setting:

```code
    inputs: explain circa question: Do you see yourself raising a family in New York? answer: I will never have a family.
                            choice: Yes
                            choice: Yes, subject to some conditions
                            choice: No
                            choice: In the middle, neither yes nor no
                            choice: NA
                            choice: Other
    targets: No explanation: Something here...
```

Metrics

- Accuracy
- Weighted F1
